{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from nordvpn_switcher import initialize_VPN,rotate_VPN,terminate_VPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://datarade.ai'\n",
    "entry_url = 'https://datarade.ai/search/products?keywords=&page=1&search_context=products&search_type=navbar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(entry_url)\n",
    "# click the accept cookies button\n",
    "time.sleep(1)\n",
    "accept_cookies_button = driver.find_element(By.XPATH, '//button[contains(text(), \"Accept all\")]')\n",
    "accept_cookies_button.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article_text(text):\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract relevant urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datarade_data_to_scrap = []\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "page_count = int(soup.find_all('a', class_='item')[-4].text)\n",
    "\n",
    "for page_i in range(page_count+1):\n",
    "    print(f'Scrapping {page_i + 1} / {page_count + 1} pages', end='\\r')\n",
    "    driver.get(f'https://datarade.ai/search/products?keywords=&page={page_i}&search_context=products&search_type=navbar')\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    product_list = soup.find_all(lambda x: x.text == 'View Product')\n",
    "    dataset_list = soup.find_all(lambda x: x.text == 'View Dataset')\n",
    "    for product_tag in product_list:\n",
    "        datarade_data_to_scrap.append({\n",
    "            'url': base_url + product_tag['href'],\n",
    "            'type': 'product'\n",
    "            })\n",
    "    for dataset_tag in dataset_list:\n",
    "        datarade_data_to_scrap.append({\n",
    "            'url': base_url + dataset_tag['href'],\n",
    "            'type': 'dataset'\n",
    "            })\n",
    "\n",
    "    \n",
    "# about 16 minutes computing time => consider running this asynchronously for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in datarade_data_to_scrap:\n",
    "    data['is_scrapped'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datarade_data_to_scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('datarade_data_to_scrap.json', 'w') as file:\n",
    "    json.dump(datarade_data_to_scrap, file)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract data from pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is if a file is to be finished scrapping\n",
    "with open('datarade_scrapped_data.json', 'r') as f:\n",
    "  datarade_data_to_scrap = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4423 rows total\n",
      "4358 scrapped\n",
      "65 to scrap\n"
     ]
    }
   ],
   "source": [
    "print(str(len(datarade_data_to_scrap)) + ' rows total')\n",
    "print(str(len([x for x in datarade_data_to_scrap if x['is_scrapped']])) + ' scrapped')\n",
    "print(str(len([x for x in datarade_data_to_scrap if not x['is_scrapped']])) + ' to scrap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purpose only\n",
    "\n",
    "# datasets_example_url = 'https://datarade.ai/datasets/properties-for-sale-in-uae'\n",
    "# product_example_url = 'https://datarade.ai/data-products/gain-dynamics-insurance-consumer-behaviour-data-2m-house-gain-dynamics'\n",
    "# product_example_url_1 = 'https://datarade.ai/data-products/b2b-marketing-dataset-grepsr-grepsr'\n",
    "# product_example_url_2 = 'https://datarade.ai/data-products/veraset-movement-europe-gps-mobile-location-data-reli-veraset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purpose only\n",
    "# driver.get(product_example_url_2)\n",
    "# page_source = driver.page_source\n",
    "# soup = BeautifulSoup(page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou're using Windows.\n",
      "Performing system check...\n",
      "###########################\n",
      "\u001b[0m\n",
      "NordVPN installation check: \u001b[92m✓\u001b[0m\n",
      "NordVPN service check: \u001b[92m✓\u001b[0m\n",
      "Opening NordVPN app and disconnecting if necessary...\n",
      "NordVPN app launched: \u001b[92m✓\u001b[0m\n",
      "#####################################\n",
      "\n",
      "You've entered a list of connection options. Checking list...\n",
      "\n",
      "\n",
      "Done!\n",
      "\n",
      "\n",
      "Your current ip-address is: 86.246.230.126\n",
      "\n",
      "\u001b[34mConnecting you to Germany ...\n",
      "\u001b[0m\n",
      "your new ip-address is: 45.129.35.177\n",
      "\n",
      "Done! Enjoy your new server.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "settings = initialize_VPN(area_input=['Europe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for product data\n",
    "\n",
    "def extract_from_product_page(soup):\n",
    "    data_out = {}\n",
    "    data_out['title'] = clean_article_text(soup.find(class_='product-hero__header-title-name').text)\n",
    "    if soup.find('span', class_='provider__rating-summary-score'):\n",
    "        data_out['provider__rating-summary-score'] = soup.find('span', class_='provider__rating-summary-score').text\n",
    "    if soup.find('span', class_='provider__rating-summary-count'):\n",
    "        data_out['provider__rating-summary-count'] = soup.find('span', class_='provider__rating-summary-count').text[1:-1]\n",
    "\n",
    "    data_out['dataset__fact'] = []\n",
    "    for dataset_fact_tag in soup.find_all('div', class_='dataset__fact'):\n",
    "        dataset_fact_name = clean_article_text(dataset_fact_tag.find('div', class_='dataset__fact-name').text)\n",
    "        dataset_fact_value = clean_article_text(dataset_fact_tag.find('div', class_='dataset__fact-value').text)\n",
    "        dataset_fact_label = clean_article_text(dataset_fact_tag.find('div', class_='dataset__fact-label').text)\n",
    "        data_out['dataset__fact'].append({\n",
    "            'name': dataset_fact_name,\n",
    "            'value': dataset_fact_value,\n",
    "            'label': dataset_fact_label\n",
    "        })\n",
    "\n",
    "    if soup.find('h2', string='Data Dictionary'):\n",
    "        data_dictionary_list = []\n",
    "        table_title_tags = soup.find('h2', string='Data Dictionary').find_next().find_all('div', class_='title')\n",
    "        table_tags = soup.find('h2', string='Data Dictionary').find_next().find_all('table')\n",
    "        for i in range(len(table_title_tags)):\n",
    "            table_title = clean_article_text(table_title_tags[i].text)\n",
    "            data_dictionary_tag = table_tags[i].find('tbody')\n",
    "            data_dictionary = []\n",
    "            for tablerow_tag in data_dictionary_tag.find_all('tr'):\n",
    "                data_dictionary_row = {}\n",
    "                splitted_tablerow_tag = tablerow_tag.find_all('td')\n",
    "                data_dictionary_row['attribute'] = clean_article_text(splitted_tablerow_tag[0].text)\n",
    "                data_dictionary_row['type'] = clean_article_text(splitted_tablerow_tag[1].text)\n",
    "                data_dictionary_row['example'] = clean_article_text(splitted_tablerow_tag[2].text)\n",
    "                data_dictionary_row['mapping'] = clean_article_text(splitted_tablerow_tag[3].text)\n",
    "                data_dictionary.append(data_dictionary_row)\n",
    "            data_dictionary_list.append({\n",
    "                    'title': table_title,\n",
    "                    'data': data_dictionary\n",
    "                })\n",
    "        data_out['data_dictionary'] = data_dictionary_list\n",
    "    \n",
    "    data_out['details'] = clean_article_text(soup.find('h2', string='Description').find_next().text)\n",
    "\n",
    "    geo_coverage_tag = soup.find('div', class_='countries').find_all('div', class_='inline-block')\n",
    "    data_geo_coverage = []\n",
    "    for country_tag in geo_coverage_tag:\n",
    "        data_geo_coverage.append(clean_article_text(country_tag.text))\n",
    "    data_out['geographical_coverage'] = data_geo_coverage\n",
    "\n",
    "    product_content__pricing_info = []\n",
    "    for product_content__pricing_info_tag in soup.find_all('div', class_='product-content__pricing-info'):\n",
    "        product_content__pricing_info.append(product_content__pricing_info_tag.find('span').text)\n",
    "    data_out['product-content__pricing-info'] = product_content__pricing_info\n",
    "\n",
    "    if soup.find('h2', string='Pricing').find_next('table'):\n",
    "        pricing_plans = []\n",
    "        for table_tag in soup.find('h2', string='Pricing').find_next('table').find('tbody').find_all('tr'):\n",
    "            pricing_plan = {}\n",
    "            pricing_plan['license'] = table_tag.find('th').text\n",
    "            pricing_plan['starts_at'] = table_tag.find('span').text\n",
    "            pricing_plans.append(pricing_plan)\n",
    "        data_out['pricing_plans'] = pricing_plans\n",
    "\n",
    "    if soup.find('h2', string='Suitable Company Sizes'):\n",
    "        suitable_company_sizes = []\n",
    "        suitable_company_sizes_tags = soup.find('h2', string='Suitable Company Sizes').find_next().find_all(class_='checked-tag-group__item active')\n",
    "        for tag in suitable_company_sizes_tags:\n",
    "            suitable_company_sizes.append(tag.text)\n",
    "        data_out['suitable_company_sizes'] = suitable_company_sizes\n",
    "\n",
    "    if soup.find('h2', string='Quality'):\n",
    "        quality_list = []\n",
    "        quality_tags = soup.find('h2', string='Quality').find_next().find_all(class_='product-content__quality-indicator')\n",
    "        for quality_tag in quality_tags:\n",
    "            value = quality_tag.find('span').text\n",
    "            for child in quality_tag.find_all('div'):\n",
    "                child.decompose()\n",
    "            title = clean_article_text(quality_tag.text)\n",
    "            quality_list.append({\n",
    "                'value': value,\n",
    "                'title': title\n",
    "            })\n",
    "            data_out['quality'] = quality_list\n",
    "\n",
    "    if soup.find('h2', string='Delivery'):\n",
    "        delivery = []\n",
    "        for delivery_type_tag in soup.find('h2', string='Delivery').find_next().find_all(class_='product-content__delivery-header'):\n",
    "            delivery_type = delivery_type_tag.text\n",
    "            value_list = []\n",
    "            for value_tag in delivery_type_tag.find_next().find_all(class_='checked-tag-group__item active'):\n",
    "                value_list.append(value_tag.text)\n",
    "            delivery.append({\n",
    "                'type': delivery_type,\n",
    "                'values': value_list\n",
    "            })\n",
    "        data_out['delivery'] = delivery\n",
    "    \n",
    "    if soup.find('h2', string='History'):\n",
    "        data_out['history'] = clean_article_text(soup.find('h2', string='History').find_next().text)\n",
    "\n",
    "    if soup.find('h2', string='Use Cases'):\n",
    "        use_cases = []\n",
    "        for use_case_tag in soup.find('h2', string='Use Cases').find_next().find_all('span'):\n",
    "            use_cases.append(use_case_tag.text)\n",
    "        data_out['use_cases'] = use_cases\n",
    "\n",
    "    if soup.find('h2', string='Categories'):\n",
    "        categories = []\n",
    "        for category_tag in soup.find('h2', string='Categories').find_next().find_all('span'):\n",
    "            categories.append(category_tag.text)\n",
    "        data_out['categories'] = categories\n",
    "\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset data\n",
    "\n",
    "def extract_from_dataset_page(soup):\n",
    "    data_out = {}\n",
    "    data_out['title'] = clean_article_text(soup.find(class_='dataset__header-title-name').text)\n",
    "\n",
    "    data_out['provider'] = soup.find('a', class_='provider').text\n",
    "    if soup.find('span', class_='provider__rating-summary-score'):\n",
    "        data_out['provider__rating-summary-score'] = soup.find('span', class_='provider__rating-summary-score').text\n",
    "    if soup.find('span', class_='provider__rating-summary-count'):\n",
    "        data_out['provider__rating-summary-count'] = soup.find('span', class_='provider__rating-summary-count').text[1:-1]\n",
    "\n",
    "    data_out['dataset__fact'] = []\n",
    "    for dataset_fact_tag in soup.find_all('div', class_='dataset__fact'):\n",
    "        dataset_fact_name = clean_article_text(dataset_fact_tag.find('div', class_='dataset__fact-name').text)\n",
    "        dataset_fact_value = clean_article_text(dataset_fact_tag.find('div', class_='dataset__fact-value').text)\n",
    "        dataset_fact_label = clean_article_text(dataset_fact_tag.find('div', class_='dataset__fact-label').text)\n",
    "        data_out['dataset__fact'].append({\n",
    "            'name': dataset_fact_name,\n",
    "            'value': dataset_fact_value,\n",
    "            'label': dataset_fact_label\n",
    "        })\n",
    "\n",
    "    if len(soup.find_all('table', class_='table--dataset')) > 1:\n",
    "        data_dictionary = []\n",
    "        data_dictionary_tag = soup.find_all('table', class_='table--dataset')[1].find('tbody')\n",
    "        for tablerow_tag in data_dictionary_tag.find_all('tr'):\n",
    "            data_dictionary_row = {}\n",
    "            splitted_tablerow_tag = tablerow_tag.find_all('td')\n",
    "            data_dictionary_row['attribute'] = clean_article_text(splitted_tablerow_tag[0].text)\n",
    "            data_dictionary_row['type'] = clean_article_text(splitted_tablerow_tag[1].text)\n",
    "            data_dictionary_row['example'] = clean_article_text(splitted_tablerow_tag[2].text)\n",
    "            data_dictionary_row['mapping'] = clean_article_text(splitted_tablerow_tag[3].text)\n",
    "            data_dictionary.append(data_dictionary_row)\n",
    "        data_out['data_dictionary'] = data_dictionary\n",
    "\n",
    "    data_out['details'] = clean_article_text(soup.find('h3', string='Details').find_next().text)\n",
    "\n",
    "    if (soup.find('div', class_='ui list')):\n",
    "        geo_coverage_tag = soup.find('div', class_='ui list').find_all('div', class_='inline-block')\n",
    "        data_geo_coverage = []\n",
    "        for country_tag in geo_coverage_tag:\n",
    "            data_geo_coverage.append(clean_article_text(country_tag.text))\n",
    "        data_out['geographical_coverage'] = data_geo_coverage\n",
    "\n",
    "    dataset__categories_tags = soup.find('div', class_='dataset__categories').find_all('span')\n",
    "    dataset__categories = []\n",
    "    for dataset__categories_tag in dataset__categories_tags:\n",
    "        dataset__categories.append(dataset__categories_tag.text)\n",
    "\n",
    "    data_out['dataset__categories'] = dataset__categories\n",
    "    dataset__price_tag = soup.find('div', class_='dataset__price')\n",
    "    for child in dataset__price_tag.find_all('div'):\n",
    "        child.decompose()\n",
    "    data_out['dataset__price'] = dataset__price_tag.text\n",
    "\n",
    "    if soup.find('h2', string='History'):\n",
    "        data_out['history'] = clean_article_text(soup.find('h2', string='History').find_next().text)\n",
    "\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4423 rows total\n",
      "4423 scrapped\n",
      "0 to scrap\n"
     ]
    }
   ],
   "source": [
    "print(str(len(datarade_data_to_scrap)) + ' rows total')\n",
    "print(str(len([x for x in datarade_data_to_scrap if x['is_scrapped']])) + ' scrapped')\n",
    "print(str(len([x for x in datarade_data_to_scrap if not x['is_scrapped']])) + ' to scrap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your current ip-address is: 85.190.239.167\n",
      "\n",
      "\u001b[34mConnecting you to Czech Republic ...\n",
      "\u001b[0m\n",
      "your new ip-address is: 185.250.215.193\n",
      "\n",
      "Done! Enjoy your new server.\n",
      "\n",
      "scrapping page 4423 / 4423 : https://datarade.ai/data-products/international-address-checker                                                                                                                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    driver.close()\n",
    "except:\n",
    "    pass\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "rotate_VPN(settings) \n",
    "\n",
    "pages_to_scrap_count = len(datarade_data_to_scrap)\n",
    "page_i = 0\n",
    "vpn_rotate_counter = 0\n",
    "for data_to_scrap in datarade_data_to_scrap:\n",
    "    page_i += 1\n",
    "    data_entry_dict = data_to_scrap\n",
    "    url = data_entry_dict['url']\n",
    "    print(f'scrapping page {page_i} / {pages_to_scrap_count} : {url}                                                         ', end='\\r')\n",
    "\n",
    "    if data_to_scrap['is_scrapped']:\n",
    "        continue \n",
    "\n",
    "    vpn_rotate_counter += 1\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        if data_entry_dict['type'] == 'product':\n",
    "            #We filter the product if unavailable\n",
    "            if soup.find(class_='landing-page__title') and clean_article_text(soup.find(class_='landing-page__title').text) == 'This product is currently unavailable':\n",
    "                data_entry_dict.update({'title': 'unavailable',\n",
    "                                        'is_scrapped': True})\n",
    "                continue\n",
    "            data_entry_dict.update(extract_from_product_page(soup))\n",
    "        elif data_entry_dict['type'] == 'dataset':\n",
    "            data_entry_dict.update(extract_from_dataset_page(soup))\n",
    "        else:\n",
    "            warnings.warn(f'{url} has no correct type')\n",
    "        data_to_scrap['is_scrapped'] = True\n",
    "    except Exception as e:\n",
    "        print(f'Couldn\\'t scrap {url}')\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    if vpn_rotate_counter >= 40:\n",
    "        rotate_VPN(settings) \n",
    "        driver.close()\n",
    "        driver = webdriver.Chrome()\n",
    "        vpn_rotate_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datarade_scrapped_data.json', 'w') as file:\n",
    "    json.dump(datarade_data_to_scrap, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrap use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO :scrap also those links : https://datarade.ai/use-cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
